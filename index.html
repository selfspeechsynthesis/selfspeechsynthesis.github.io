
<!DOCTYPE html>
<html>
<head>
	<meta charset="UTF-8">

	<title>SelfVC</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
	<!-- Latest compiled and minified Bootstrap CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

	<link rel="stylesheet" type="text/css" href="style_examples.css">

</head>
<body>

	
	
	


	<div class="container">
		
		<h1>SelfVC: Voice Conversion With Iterative Refinement using Self Transformations</h1>
		<h3><a href="https://selfspeechsynthesis.github.io/demo.html">Interactive Demo</a></h3>
        
		<!-- <div style="border: 1px solid black; margin-top: 20px; margin-bottom: 10px;"></div> -->
		<div style="border-top: 1px solid grey; margin-top: 20px; margin-bottom: 10px;"></div>

		<p>We present audio examples for our paper <i>SelfVC: Voice Conversion With Iterative Refinement using Self Transformations</i>. To perform zero-shot voice conversion, we use our synthesis model to combine the content embedding of any given source utterance with the speaker embedding of the target speaker derived from a speaker verification model using 10 seconds of audio of the target speaker. Our synthesizer can perform voice conversion in two modes:</p>
		<ul>
			<li><b>Guided:</b> In this setting, the prosody (speaking rate and pitch modulation) of the synthesized speech matches the prosody of the source utterance. To achieve this, we use the ground-truth pitch contour (derived from Yin algorithm) and durations (derived by grouping the content representations) during synthesis.</li>
			<li><b>Predictive:</b> In this setting, the prosody of the synthesized speech is derived from both the source utterance and target speaker's audio. We predict the normalized pitch contour and durations of SRE embeddings based on both the content and speaker embeddings. </li>
		</ul>
		<div style="border-top: 1px solid grey;"></div>
		<div class="row">
			
			<h3>Zero-shot Any-to-Any Voice Convesion</h3>
			
			<p>
				To perform voice conversion for speakers not seen during training, we randomly select 10 male and 10 female speakers from the <i>test-clean</i> subset of the LibriTTS dataset as our target speakers. Next, we choose 10 random source utterances from the remaining speakers and perform voice conversion for each of the 20 speakers. We present a few audio examples for this experiment in the table below. 
			</p>
			<table class="table" style="margin-top: 20px;">
				<thead>
					<tr>
						<th>Conversion Type</th>
						<th>Source Utterance</th>
						<th>Target Speaker</th>
						<th><b>SelfVC (Predictive)</b></th>
						<th><b>SelfVC (Guided)</b></th>
					</tr>
				</thead>
				<tbody id = "unseen_speakers" >
					
				</tbody>
			</table>
		</div>
		
		<!-- 
		<div style="border-top: 1px solid grey;"></div>
		<div class="row">
			<center>
			<h3>Voice Converstion for Seen Speakers (Many-to-Many)</h3>
			</center>
			<p>
				To perform voice conversion for seen speakers, we use the hold-out utterances of speakers seen during training. Similar to the unseen speaker scenario, we select 10 male and 10 female speakers as the target speakers and choose source utterances from other speakers. 
			</p>
			<table class="table" style="margin-top: 20px;">
				<thead>
					<tr>
						<th>Conversion Type</th>
						<th>Source Utterance</th>
						<th>Target Speaker</th>
						<th><b>SelfVC (Predictive)</b></th>
						<th><b>SelfVC (Guided)</b></th>
					</tr>
				</thead>
				<tbody id = "seen_speakers" >
					
				</tbody>
			</table>
		</div>
		 -->

		<div style="border-top: 1px solid grey;"></div>
		<div class="row">
			
			<h3>Comparison Against Past Work (Unseen Speakers)</h3>
			
			<p>We present audio examples for the same pair of source and target audio using different voice conversion techniques including our own. We use the <i>Adapt</i> mode for our technique. We produce audio examples for other techniques using the voice convesion inference script provided in the respective github repositories.</p>
			<table class="table" style="margin-top: 20px;">
				<thead>
					<tr>
						<th>Conversion Type</th>
						<th>Source Utterance</th>
						<th>Target Speaker</th>
						<th><a href="https://github.com/BrightGu/MediumVC">MediumVC</a></th>
						<th><a href="https://github.com/s3prl/s3prl">S3PRL-VC</a></th>
						<th><a href="https://github.com/Edresson/YourTTS">YourTTS</a></th>
						<th><b>Ours</b></th>
					</tr>
				</thead>
				<tbody id = "comparison_table" >
					
				</tbody>
			</table>
		</div>
		
		<div style="border-top: 1px solid grey;"></div>
		<div class="row">
			
			<h3>Cross Lingual Voice Conversion</h3>
			
			<p>
				To perform voice conversion for speakers not seen during training, we randomly select 10 male and 10 female speakers from the <i>test-clean</i> subset of the LibriTTS dataset as our target speakers. Next, we choose 10 random source utterances from the remaining speakers and perform voice conversion for each of the 20 speakers. We present a few audio examples for this experiment in the table below. 
			</p>
			<h4>English To CSS10</h4>
			<table class="table" style="margin-top: 10px; margin-bottom: 20px;">
				<thead>
					<tr>
						<th>Conversion Type</th>
						<th>Source Utterance (English) </th>
						<th>Target Speaker (CSS10)</th>
						<th><b>Ours - SelfVC (LibriTTS)</b></th>
						<th><b>Ours - SelfVC (LibriTTS + CSS10)</b></th>
					</tr>
				</thead>
				<tbody id = "crosslingual_englishTocss10" >
					
				</tbody>
			</table>

			<h4>CSS10 To English</h4>
			<table class="table" style="margin-top: 10px; margin-bottom: 20px;">
				<thead>
					<tr>
						<th>Conversion Type</th>
						<th>Source Utterance (CSS10)</th>
						<th>Target Speaker (English) </th>
						<th><b>Ours - SelfVC (LibriTTS)</b></th>
						<th><b>Ours - SelfVC (LibriTTS + CSS10)</b></th>
					</tr>
				</thead>
				<tbody id = "crosslingual_css10Toenglish" >
					
				</tbody>
			</table>

			<h4>CSS10 To CSS10</h4>
			<table class="table" style="margin-top: 10px; margin-bottom: 20px;">
				<thead>
					<tr>
						<th>Conversion Type</th>
						<th>Source Utterance (CSS10)</th>
						<th>Target Speaker (CSS10)</th>
						<th><b>Ours - SelfVC (LibriTTS)</b></th>
						<th><b>Ours - SelfVC (LibriTTS + CSS10)</b></th>
					</tr>
				</thead>
				<tbody id = "crosslingual_css10Tocss10" >
					
				</tbody>
			</table>


		</div>

		<!-- 
		<div style="border-top: 1px solid grey;"></div>
		<div class="row">
			<center>
			<h3>Expressive source utterances (Unseen speakers)</h3>
			</center>
			<p>We present audio examples where source utterances are from expressive/emotional speakers. We use the <a href="https://zenodo.org/record/5117102#.Y2RS6OzMKvg">ADEPT dataset</a> for these examples. The source utterances are from the expressive audio of the two speakers in the dataset. The neutral utterances are used for deriving the speaker embedding. Both the male and female speakers are not seen during training.</p>
			<table class="table" style="margin-top: 20px;">
				<thead>
					<tr>
						<th>Conversion Type</th>
						<th>Source Utterance</th>
						<th>Target Speaker</th>
						<th><b>Ours (Predictive)</b></th>
						<th><b>Ours (Guided)</b></th>
					</tr>
				</thead>
				<tbody id = "emotional_speakers" >
					
				</tbody>
			</table>
		</div>
		-->

		<div style="border-top: 1px solid grey;"></div>
		<div class="row">
			
			<h3>Manual Prosody Control</h3>
			
			<p>
				Besides the above two inference modes (guided and predictive), SelfVC also offers fine-grained control over the prosody of the synthesized speech.
				During inference, we can simply modify the pitch contour and duration of the source utterance to control the prosody of the synthesized speech. 
				We present audio examples obtained by scaling the reference pitch contour and duration by a factor. This behaviour is similar to ACE-VC, except 
				that we do not require any transcriptions during training to train our duration predictor.
			</p>

			<h4>Pace Control</h4>
			<table class="table" style="margin-top: 10px; margin-bottom: 30px">
				<thead>
					<tr>
						<th>Conversion Type</th>
						<th>Source Utterance</th>
						<th>Target Speaker</th>
						<th>Same Pace</th>
						<th>Fast Pace (1.5 X)</th>
						<th>Slow Pace (0.7 X)</th>
					</tr>
				</thead>
				<tbody id = "pace_control" >
					
				</tbody>
			</table>

			<h4>Pitch Control</h4>
			<table class="table" style="margin-top: 20px;">
				<thead>
					<tr>
						<th>Conversion Type</th>
						<th>Source Utterance</th>
						<th>Target Speaker</th>
						<th>Same Pitch (1X)</th>
						<th>Higher Pitch (3X)</th>
						<th>Lower Pitch (0.5X)</th>
					</tr>
				</thead>
				<tbody id = "pitch_control" >
					
				</tbody>
			</table>

		</div>

	</div>
	<hr>
	
</body>
	
	
	
<script type="text/javascript">

function fill_audio_table(tbody_id, audio_columns, transcripts, audio_width){
	var num_rows = audio_columns[1].length;
	var base_url = "https://expressivecloning.s3.us-east-2.amazonaws.com/AMTEVAL/"
	for(var ridx=0; ridx < num_rows; ridx++){
		var tr_string = "<tr>";
		for(var cidx=0; cidx < audio_columns.length; cidx++){
			if(cidx > 0){
				var audio_url = audio_columns[cidx][ridx];
				if(!audio_url.startsWith("audio/") ){
					audio_url = base_url + audio_columns[cidx][ridx];	
				}
				tr_string += '<td><audio class="class_audio" controls="" style="width:'+ audio_width +'px; text-align: center;"><source src="' + audio_url + '" type="audio/wav">Your browser does not support the audio tag</audio>';
				if(cidx == 1 && transcripts.length > 0){
					tr_string += '<details><summary style="cursor: pointer; color: #76b900;">[Show transcript]</summary>' + transcripts[ridx] + '</details>'
				}
				tr_string += '</td>';
				
			}
			else{
				tr_string += '<td style="white-space: nowrap; text-align: left">' + audio_columns[cidx][ridx] + '</td>';
			}
		}
		tr_string += "</tr>";
		$("#" + tbody_id).append(tr_string);
	}
	
}

function fill_audio_table_comparison(tbody_id, audio_columns, transcripts, audio_width){
	var num_rows = audio_columns[0].length;
	var base_url = "https://expressivecloning.s3.us-east-2.amazonaws.com/AMTEVAL/"
	for(var ridx=0; ridx < num_rows; ridx++){
		var tr_string = "<tr>";
		for(var cidx=0; cidx < audio_columns.length; cidx++){
			if(cidx > 0){
				var audio_url = base_url + audio_columns[cidx][ridx];
				tr_string += '<td><audio class="class_audio" controls="" style="width:'+ audio_width +'px; text-align: center;"><source src="' + audio_url + '" type="audio/wav">Your browser does not support the audio tag</audio>';
				if(cidx == 1){
					tr_string += '<details><summary style="cursor: pointer; color: #76b900;">[Show transcript]</summary>' + transcripts[ridx] + '</details>'
				}
				tr_string += '</td>';	
			}
			else{
				tr_string += '<td style="white-space: nowrap; text-align: left">' + audio_columns[cidx][ridx] + '</td>';
			}
			
		}
		tr_string += "</tr>";
		$("#" + tbody_id).append(tr_string);
	}
	
}

var comparison_types_unseen = [
]

var unseen_transcripts = [
	"Going back to camp I procured a light, and after hooping and hallowing for a long time, I heard another groan. This time much louder than before.",
	"When quite crisp, they are ready for use.",
	"Various dishes are frequently ornamented and garnished with its graceful leaves and these are sometimes boiled in soups. Although it is more usually confined to English cookery...",
	"Going back to camp I procured a light, and after hooping and hallowing for a long time, I heard another groan. This time much louder than before.",
	"Going back to camp I procured a light, and after hooping and hallowing for a long time, I heard another groan. This time much louder than before.",
	"Going back to camp I procured a light, and after hooping and hallowing for a long time, I heard another groan. This time much louder than before.",
	"Going back to camp I procured a light, and after hooping and hallowing for a long time, I heard another groan. This time much louder than before.",
]


var female_source_utterances = [
	"source_110",
	// "source_70"
]

var male_source_utterances = [
	"source_140",
	// "source_150"
]

var female_target_speakers = [
	"targetspeaker_29",
	// "targetspeaker_29",
]

var male_target_speakers = [
	"targetspeaker_34",
	// "targetspeaker_34"
]

var source_utterances_unseen = [];
var target_utterances_unseen = [];
var mimic_utterances_unseen = [];
var adapt_utterances_unseen = [];

for(var fti=0; fti < female_target_speakers.length; fti++){
	for(var fsi=0; fsi < female_source_utterances.length; fsi++){
		var source = female_source_utterances[fsi]
		var target = female_target_speakers[fti]
		var source_path = "SRC_UNSEEN_LIBRI/" + source + ".wav";
		var target_path = "TARGET_UNSEEN_LIBRI/" + target + ".wav";
		var mimic_path = "unseenlibri_guidedself/" + source + "_" + target + ".wav";
		var adapt_path = "unseenlibri_predself/" + source + "_" + target + ".wav";
		source_utterances_unseen.push(source_path);
		target_utterances_unseen.push(target_path);
		mimic_utterances_unseen.push(mimic_path);
		adapt_utterances_unseen.push(adapt_path);
		comparison_types_unseen.push("Female To Female");
	}
	for(var msi=0; msi < male_source_utterances.length; msi++){
		var source = male_source_utterances[msi]
		var target = female_target_speakers[fti]
		var source_path = "SRC_UNSEEN_LIBRI/" + source + ".wav";
		var target_path = "TARGET_UNSEEN_LIBRI/" + target + ".wav";
		var mimic_path = "unseenlibri_guidedself/" + source + "_" + target + ".wav";
		var adapt_path = "unseenlibri_predself/" + source + "_" + target + ".wav";
		source_utterances_unseen.push(source_path);
		target_utterances_unseen.push(target_path);
		mimic_utterances_unseen.push(mimic_path);
		adapt_utterances_unseen.push(adapt_path);
		comparison_types_unseen.push("Male To Female");
	}
}


for(var mti=0; mti < male_target_speakers.length; mti++){
	for(var fsi=0; fsi < female_source_utterances.length; fsi++){
		var source = female_source_utterances[fsi]
		var target = male_target_speakers[mti]
		var source_path = "SRC_UNSEEN_LIBRI/" + source + ".wav";
		var target_path = "TARGET_UNSEEN_LIBRI/" + target + ".wav";
		var mimic_path = "unseenlibri_guidedself/" + source + "_" + target + ".wav";
		var adapt_path = "unseenlibri_predself/" + source + "_" + target + ".wav";
		source_utterances_unseen.push(source_path);
		target_utterances_unseen.push(target_path);
		mimic_utterances_unseen.push(mimic_path);
		adapt_utterances_unseen.push(adapt_path);
		comparison_types_unseen.push("Female To Male");
	}
	for(var msi=0; msi < male_source_utterances.length; msi++){
		var source = male_source_utterances[msi];
		var target = male_target_speakers[mti];
		var source_path = "SRC_UNSEEN_LIBRI/" + source + ".wav";
		var target_path = "TARGET_UNSEEN_LIBRI/" + target + ".wav";
		var mimic_path = "unseenlibri_guidedself/" + source + "_" + target + ".wav";
		var adapt_path = "unseenlibri_predself/" + source + "_" + target + ".wav";
		source_utterances_unseen.push(source_path);
		target_utterances_unseen.push(target_path);
		mimic_utterances_unseen.push(mimic_path);
		adapt_utterances_unseen.push(adapt_path);
		comparison_types_unseen.push("Male To Male");
	}
}

var unseen_columns = [
	comparison_types_unseen,
	source_utterances_unseen,
	target_utterances_unseen,
	// pitch_pred_utterances_unseen,
	adapt_utterances_unseen,
	mimic_utterances_unseen,
]


var source_utterances_comparison = [
	"source_110",
	"source_70",
	"source_140",
	"source_150"

]

var target_utterances_comparison = [
	"targetspeaker_29",
	"targetspeaker_29",
	"targetspeaker_29",
	"targetspeaker_34",
]

var comparison_types = [
	"Female To Female",
	"Female To Female",
	"Male To Male",
	"Male To Male",
]

var comparison_transcripts = [
	"dummy",
	"dummy",
	"dummy",
	"dummy",
]

var source_paths = [];
var target_paths = [];
var fragment_comparison = [];
var s2vc_comparison = [];
var yourtts_comparison = [];
var ace_comparison = [];
var ours_comparison = [];

for(var idx=0; idx < source_utterances_comparison.length; idx++){
	source_paths.push("SRC_UNSEEN_LIBRI/" + source_utterances_comparison[idx] + ".wav");
	target_paths.push("TARGET_UNSEEN_LIBRI/" + target_utterances_comparison[idx] + ".wav");
	fragment_comparison.push("unseenlibri_fragmentvc" + "/" + source_utterances_comparison[idx] + "_" + target_utterances_comparison[idx] + ".wav");
	s2vc_comparison.push("unseenlibri_s2vc" + "/" + source_utterances_comparison[idx] + "_" + target_utterances_comparison[idx] + ".wav");
	yourtts_comparison.push("unseenlibri_yourtts" + "/" + source_utterances_comparison[idx] + "_" + target_utterances_comparison[idx] + ".wav");
	ace_comparison.push("unseenlibri_ace" + "/" + source_utterances_comparison[idx] + "_" + target_utterances_comparison[idx] + ".wav");
	ours_comparison.push("unseenlibri_self" + "/" + source_utterances_comparison[idx] + "_" + target_utterances_comparison[idx] + ".wav");
}

var comparison_columns = [
	comparison_types,
	source_paths,
	target_paths,
	fragment_comparison,
	s2vc_comparison,
	yourtts_comparison,
	ace_comparison,
	ours_comparison
]

fill_audio_table("unseen_speakers", unseen_columns,unseen_transcripts, 250)
// fill_audio_table("seen_speakers", seen_columns, seen_transcripts, 250)
fill_audio_table_comparison("comparison_table", comparison_columns, comparison_transcripts, 170)

var css_source_language_pairs = [
	["source_00", "chinese"],
	["source_10", "greek"],
	["source_20", "finnish"],
	["source_30", "spanish"],
	["source_40", "dutch"],
	["source_50", "german"],
	["source_60", "japanese"],
	["source_70", "french"],
	["source_80", "hungarian"],
	["source_90", "russian"],
]

var english_source_utterances = [
	"source_110",
	"source_110",
	"source_110",
	"source_110",
	"source_110",
	"source_140",
	"source_140",
	"source_140",
	"source_140",
	"source_140",
]

var css_target_language_pairs = [
	["targetspeaker_1_1", "greek"],
	["targetspeaker_2_1", "finnish"],
	["targetspeaker_3_1", "spanish"],
	["targetspeaker_4_1", "dutch"],
	["targetspeaker_5_1", "german"],
	["targetspeaker_6_1", "japanese"],
	["targetspeaker_7_1", "french"],
	["targetspeaker_8_1", "hungarian"],
	["targetspeaker_9_1", "russian"],
	["targetspeaker_0_1", "chinese"],
]

var english_target_speakers = [
	"targetspeaker_29",
	"targetspeaker_35",
	"targetspeaker_23",
	"targetspeaker_30",
	"targetspeaker_36",
	"targetspeaker_34",
	"targetspeaker_31",
	"targetspeaker_29",
	"targetspeaker_35",
	"targetspeaker_23",
]

var cross_lingual_types = [];
var cross_lingual_sources = [];
var cross_lingual_targets = [];
var cross_lingual_self = [];
var cross_lingual_selfCSS = [];
for(var esi=0; esi < english_source_utterances.length; esi++){
	var source = english_source_utterances[esi];
	var target = css_target_language_pairs[esi][0];
	var source_path = "SRC_UNSEEN_LIBRI/" + source + ".wav";
	var target_path = "TARGET_CSS_ALL/" + target + ".wav";
	var self_path = "LibriTocss10_self/" + source + "_" + target + ".wav";
	var selfCSS_path = "LibriTocss10_selfCSS/" + source + "_" + target + ".wav";
	cross_lingual_types.push("English To " + css_target_language_pairs[esi][1]);
	cross_lingual_sources.push(source_path);
	cross_lingual_targets.push(target_path);
	cross_lingual_self.push(self_path);
	cross_lingual_selfCSS.push(selfCSS_path);
}

var cross_lingual_columns = [
	cross_lingual_types,
	cross_lingual_sources,
	cross_lingual_targets,
	cross_lingual_self,
	cross_lingual_selfCSS,
]

fill_audio_table("crosslingual_englishTocss10", cross_lingual_columns, [], 200);


cross_lingual_types = [];
cross_lingual_sources = [];
cross_lingual_targets = [];
cross_lingual_self = [];
cross_lingual_selfCSS = [];
for(var esi=0; esi < english_target_speakers.length; esi++){
	var source = css_source_language_pairs[esi][0];
	var target = english_target_speakers[esi];
	var source_path = "SRC_CSS_ALL/" + source + ".wav";
	var target_path = "TARGET_UNSEEN_LIBRI/" + target + ".wav";
	var self_path = "css10ToLibri_self/" + source + "_" + target + ".wav";
	var selfCSS_path = "css10ToLibri_selfCSS/" + source + "_" + target + ".wav";
	cross_lingual_types.push(css_source_language_pairs[esi][1] + " To English");
	cross_lingual_sources.push(source_path);
	cross_lingual_targets.push(target_path);
	cross_lingual_self.push(self_path);
	cross_lingual_selfCSS.push(selfCSS_path);
}

var cross_lingual_columns = [
	cross_lingual_types,
	cross_lingual_sources,
	cross_lingual_targets,
	cross_lingual_self,
	cross_lingual_selfCSS,
]

fill_audio_table("crosslingual_css10Toenglish", cross_lingual_columns, [], 200);



cross_lingual_types = [];
cross_lingual_sources = [];
cross_lingual_targets = [];
cross_lingual_self = [];
cross_lingual_selfCSS = [];
for(var esi=0; esi < english_target_speakers.length; esi++){
	var source = css_source_language_pairs[esi][0];
	var target = css_target_language_pairs[esi][0];
	var source_path = "SRC_CSS_ALL/" + source + ".wav";
	var target_path = "TARGET_CSS_ALL/" + target + ".wav";
	var self_path = "css10Tocss10_self/" + source + "_" + target + ".wav";
	var selfCSS_path = "css10Tocss10_selfCSS/" + source + "_" + target + ".wav";
	cross_lingual_types.push(css_source_language_pairs[esi][1] + " To " + css_target_language_pairs[esi][1]);
	cross_lingual_sources.push(source_path);
	cross_lingual_targets.push(target_path);
	cross_lingual_self.push(self_path);
	cross_lingual_selfCSS.push(selfCSS_path);
}

var cross_lingual_columns = [
	cross_lingual_types,
	cross_lingual_sources,
	cross_lingual_targets,
	cross_lingual_self,
	cross_lingual_selfCSS,
]

fill_audio_table("crosslingual_css10Tocss10", cross_lingual_columns, [], 200);



var pitch_control_types = [
	"Female to Male",
	"Male to Female",
]

var source_pitch_control = [
	'source_110',
	'source_140',
]

var target_pitch_control = [
	"targetspeaker_29",
	"targetspeaker_34",
]

var prosody_source_paths = [];
var prosody_target_paths = [];
var prosody_normal_paths = [];
var prosody_high_paths = [];
var prosody_low_paths = [];
for(var idx=0; idx < source_pitch_control.length; idx++){
	prosody_source_paths.push("SRC_UNSEEN_LIBRI/" + source_pitch_control[idx] + ".wav");
	prosody_target_paths.push("TARGET_UNSEEN_LIBRI/" + target_pitch_control[idx] + ".wav");
	prosody_normal_paths.push("unseenlibri_self/" + source_pitch_control[idx] + "_" + target_pitch_control[idx] + ".wav");
	prosody_high_paths.push("unseenlibri_self_pace1.5/" + source_pitch_control[idx] + "_" + target_pitch_control[idx] + ".wav");
	prosody_low_paths.push("unseenlibri_self_pace0.7/" + source_pitch_control[idx] + "_" + target_pitch_control[idx] + ".wav");
}

var control_columns = [
	pitch_control_types,
	prosody_source_paths,
	prosody_target_paths,
	prosody_normal_paths,
	prosody_high_paths,
	prosody_low_paths,
]

var transcripts_pace_control = [
	"Various dishes are frequently ornamented and garnished with its graceful leaves and these are sometimes boiled in soups. Although it is more usually confined to English cookery...",
	"Going back to camp I procured a light, and after hooping and hallowing for a long time, I heard another groan. This time much louder than before.",
	
]

fill_audio_table("pace_control", control_columns, transcripts_pace_control, 200)



prosody_source_paths = [];
prosody_target_paths = [];
prosody_normal_paths = [];
prosody_high_paths = [];
prosody_low_paths = [];
for(var idx=0; idx < source_pitch_control.length; idx++){
	prosody_source_paths.push("SRC_UNSEEN_LIBRI/" + source_pitch_control[idx] + ".wav");
	prosody_target_paths.push("TARGET_UNSEEN_LIBRI/" + target_pitch_control[idx] + ".wav");
	prosody_normal_paths.push("unseenlibri_guidedself/" + source_pitch_control[idx] + "_" + target_pitch_control[idx] + ".wav");
	prosody_high_paths.push("unseenlibri_self_pitch3x/" + source_pitch_control[idx] + "_" + target_pitch_control[idx] + ".wav");
	prosody_low_paths.push("unseenlibri_self_pitch0.5x/" + source_pitch_control[idx] + "_" + target_pitch_control[idx] + ".wav");
}

var control_columns = [
	pitch_control_types,
	prosody_source_paths,
	prosody_target_paths,
	prosody_normal_paths,
	prosody_high_paths,
	prosody_low_paths,
]

fill_audio_table("pitch_control", control_columns, transcripts_pace_control, 200)

</script>	


</html>
